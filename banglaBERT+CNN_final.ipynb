{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "# Load training and test datasets from Google Drive\n",
    "train_df = pd.read_csv('/content/drive/MyDrive/dataset/train2 (1).csv', encoding='utf-8', encoding_errors='ignore')\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/dataset/test2 (1).csv', encoding='utf-8', encoding_errors='ignore')\n",
    "\n",
    "print('Number of training sentences: {:,}'.format(train_df.shape[0]))\n",
    "print('Number of test sentences: {:,}'.format(test_df.shape[0]))\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nTraining data:\")\n",
    "print(train_df.head(3))\n",
    "print(\"\\nTest data:\")\n",
    "print(test_df.head(3))\n",
    "\n",
    "# Rename columns to standardize\n",
    "train_df.rename(columns={'Sentiment':'label', 'Data':'Text'}, inplace=True)\n",
    "test_df.rename(columns={'Sentiment':'label', 'Data':'Text'}, inplace=True)\n",
    "\n",
    "# Keep only necessary columns\n",
    "train_df = train_df[['label', 'Text']]\n",
    "test_df = test_df[['label', 'Text']]\n",
    "\n",
    "print(f\"\\nTraining data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nTraining set label distribution:\")\n",
    "print(train_df.label.value_counts())\n",
    "print(\"\\nTest set label distribution:\")\n",
    "print(test_df.label.value_counts())\n",
    "\n",
    "# Plot training label distribution\n",
    "train_df.label.value_counts().plot(kind='bar', title='Training Set Label Distribution')\n",
    "plt.show()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Preprocessing patterns\n",
    "urlPattern = r\"((http://)[^ ]*|(https://)[^ ]*|(www\\.)[^ ]*)\"\n",
    "userPattern = '@[^\\s]+'\n",
    "hashtagPattern = '#[^\\s]+'\n",
    "sequencePattern = r\"(.)\\1\\1+\"\n",
    "seqReplacePattern = r\"\\1\\1\"\n",
    "\n",
    "def preprocess_apply(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    # Replace all URls with ''\n",
    "    tweet = re.sub(urlPattern,'',tweet)\n",
    "    # Replace @USERNAME to ''.\n",
    "    tweet = re.sub(userPattern,'', tweet)\n",
    "    # Replace 3 or more consecutive letters by 2 letter.\n",
    "    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "    # Adding space on either side of '/' to seperate words (After replacing URLS).\n",
    "    tweet = re.sub(r'/', ' / ', tweet)\n",
    "    return tweet\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Add word count for analysis\n",
    "train_df['word_count'] = train_df['Text'].apply(lambda x: len(str(x).split()))\n",
    "test_df['word_count'] = test_df['Text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Plot word count distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.violinplot(x=train_df['word_count'])\n",
    "plt.title('Training Set: Word Count Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.violinplot(x=test_df['word_count'])\n",
    "plt.title('Test Set: Word Count Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training set word count statistics:\")\n",
    "print(train_df.word_count.describe())\n",
    "print(\"\\nTest set word count statistics:\")\n",
    "print(test_df.word_count.describe())\n",
    "\n",
    "# Install normalizer\n",
    "!pip install git+https://github.com/csebuetnlp/normalizer\n",
    "\n",
    "from transformers import AutoModelForPreTraining, AutoTokenizer\n",
    "from normalizer import normalize\n",
    "import torch\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "print('Loading BERT tokenizer...')\n",
    "model_path = \"csebuetnlp/banglabert_large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Normalize text data\n",
    "print(\"Normalizing training data...\")\n",
    "train_df['Text'] = train_df['Text'].progress_apply(normalize)\n",
    "print(\"Normalizing test data...\")\n",
    "test_df['Text'] = test_df['Text'].progress_apply(normalize)\n",
    "\n",
    "print(\"Normalization completed!\")\n",
    "print(\"\\nTraining data after normalization:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nTest data after normalization:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Create validation split from training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "\n",
    "# Verify class distribution\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(train_data['label'].value_counts())\n",
    "print(\"\\nValidation set class distribution:\")\n",
    "print(val_data['label'].value_counts())\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(test_df['label'].value_counts())\n",
    "\n",
    "# Extract sentences and labels\n",
    "train_sentences = train_data.Text.values\n",
    "train_labels = train_data.label.values\n",
    "\n",
    "val_sentences = val_data.Text.values\n",
    "val_labels = val_data.label.values\n",
    "\n",
    "test_sentences = test_df.Text.values\n",
    "test_labels = test_df.label.values\n",
    "\n",
    "print(f\"Training: {len(train_sentences)} sentences, {len(train_labels)} labels\")\n",
    "print(f\"Validation: {len(val_sentences)} sentences, {len(val_labels)} labels\")\n",
    "print(f\"Test: {len(test_sentences)} sentences, {len(test_labels)} labels\")\n",
    "\n",
    "# Test tokenization\n",
    "print('\\nTokenization example:')\n",
    "print('Original:', test_sentences[0])\n",
    "print('Tokenized:', tokenizer.tokenize(test_sentences[0]))\n",
    "print('Token IDs:', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(test_sentences[0])))\n",
    "\n",
    "# Example with Bangla text\n",
    "demo_text = 'আমি তোমাকে ভালবাসি।'\n",
    "print(f'\\nDemo - Original: {demo_text}')\n",
    "print('Tokenized:', tokenizer.tokenize(demo_text))\n",
    "print('Token IDs:', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(demo_text)))\n",
    "\n",
    "# Label mapping - Updated to positive/negative\n",
    "label_map = {\n",
    "    0: 'positive',\n",
    "    1: 'negative'\n",
    "}\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "# Tokenize training data\n",
    "print(\"Tokenizing training data...\")\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in tqdm(train_sentences):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_len,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "# Tokenize validation data\n",
    "print(\"Tokenizing validation data...\")\n",
    "val_input_ids = []\n",
    "val_attention_masks = []\n",
    "\n",
    "for sent in tqdm(val_sentences):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_len,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                   )\n",
    "\n",
    "    val_input_ids.append(encoded_dict['input_ids'])\n",
    "    val_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "val_input_ids = torch.cat(val_input_ids, dim=0)\n",
    "val_attention_masks = torch.cat(val_attention_masks, dim=0)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Tokenize test data\n",
    "print(\"Tokenizing test data...\")\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "\n",
    "for sent in tqdm(test_sentences):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens=True,\n",
    "                        max_length=max_len,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True,\n",
    "                        return_tensors='pt',\n",
    "                   )\n",
    "\n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "print(\"Tokenization completed!\")\n",
    "\n",
    "# Create datasets\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, train_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16  # Reduced batch size for better memory management\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_dataloader)}\")\n",
    "print(f\"Validation batches: {len(validation_dataloader)}\")\n",
    "print(f\"Test batches: {len(test_dataloader)}\")\n",
    "\n",
    "# Custom BanglaBERT with CNN Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig\n",
    "import os\n",
    "\n",
    "class CustomBanglaBertWithCNN(nn.Module):\n",
    "    def __init__(self, model_name=model_path, num_labels=2, num_filters=256, filter_sizes=[3, 4, 5], dropout_rate=0.1):\n",
    "        super(CustomBanglaBertWithCNN, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        # Load BanglaBERT model and its configuration\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.num_labels = num_labels\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        # CNN layers with different filter sizes\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=self.config.hidden_size,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=filter_size\n",
    "            ) for filter_size in filter_sizes\n",
    "        ])\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(len(filter_sizes) * num_filters, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        # Forward pass through BanglaBERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "\n",
    "        # Get the sequence output (last hidden states)\n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "\n",
    "        # Transpose for Conv1d: [batch_size, hidden_size, seq_len]\n",
    "        sequence_output = sequence_output.transpose(1, 2)\n",
    "\n",
    "        # Apply convolution operations\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            # Conv1d + ReLU + MaxPooling\n",
    "            conv_out = torch.relu(conv(sequence_output))  # [batch_size, num_filters, conv_seq_len]\n",
    "            pooled = torch.max_pool1d(conv_out, kernel_size=conv_out.size(2))  # [batch_size, num_filters, 1]\n",
    "            conv_outputs.append(pooled.squeeze(2))  # [batch_size, num_filters]\n",
    "\n",
    "        # Concatenate all conv outputs\n",
    "        cnn_output = torch.cat(conv_outputs, dim=1)  # [batch_size, len(filter_sizes) * num_filters]\n",
    "\n",
    "        # Apply dropout\n",
    "        cnn_output = self.dropout(cnn_output)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(cnn_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "    def save_pretrained(self, output_dir):\n",
    "        \"\"\"Save the model weights and configuration.\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        model_to_save = self.module if hasattr(self, 'module') else self\n",
    "        torch.save(model_to_save.state_dict(), os.path.join(output_dir, \"pytorch_model.bin\"))\n",
    "        self.config.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, model_name=\"csebuetnlp/banglabert_large\", *model_args, **kwargs):\n",
    "        \"\"\"Load a model from a pretrained model.\"\"\"\n",
    "        config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n",
    "        model = cls(model_name=model_name, num_labels=config.num_labels, **kwargs)\n",
    "        model.load_state_dict(torch.load(os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")))\n",
    "        return model\n",
    "\n",
    "# Load custom model for sequence classification\n",
    "model = CustomBanglaBertWithCNN(\n",
    "    model_name=model_path,\n",
    "    num_labels=2,  # Binary classification\n",
    "    num_filters=256,\n",
    "    filter_sizes=[3, 4, 5],\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "print(\"BanglaBERT Large with CNN model loaded successfully!\")\n",
    "\n",
    "# Test the model with a sample\n",
    "sentence = \"আমি কৃতজ্ঞ কারণ আপনি আমার জন্য অনেক কিছু করেছেন।\"\n",
    "normalized_sentence = normalize(sentence)\n",
    "inputs = tokenizer(normalized_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Move inputs to device\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "# Forward pass with dummy labels\n",
    "labels = torch.tensor([1]).to(device)\n",
    "outputs = model(**inputs, labels=labels)\n",
    "print(\"Model test successful!\")\n",
    "print(f\"Sample output: {outputs}\")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=2e-5,\n",
    "                  eps=1e-8)\n",
    "\n",
    "epochs = 3  # Changed to 3 epochs\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=int(0.1 * total_steps),\n",
    "                                            num_training_steps=total_steps)\n",
    "\n",
    "# Training functions\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def train_epoch(model, train_dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=\"Training\"):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        result = model(b_input_ids,\n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels)\n",
    "\n",
    "        loss = result['loss']\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if (step + 1) % 500 == 0:\n",
    "            print(f\"Batch {step + 1}/{len(train_dataloader)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Training completed in {epoch_time // 60:.0f}m {epoch_time % 60:.0f}s\")\n",
    "    return avg_train_loss\n",
    "\n",
    "def validate_epoch(model, validation_dataloader, device):\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    total_eval_accuracy = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in tqdm(validation_dataloader, total=len(validation_dataloader), desc=\"Validating\"):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = model(b_input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels)\n",
    "\n",
    "        loss = result['loss']\n",
    "        logits = result['logits']\n",
    "\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    validation_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Validation took: {validation_time // 60:.0f}m {validation_time % 60:.0f}s\")\n",
    "    return avg_val_loss, avg_val_accuracy\n",
    "\n",
    "def save_best_model(model, tokenizer, output_dir, best_val_loss, avg_val_loss):\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        print(f\"  New best validation loss: {best_val_loss:.4f}. Saving model...\")\n",
    "\n",
    "        # Get the model to save\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "\n",
    "        # Make all parameters contiguous before saving\n",
    "        for param in model_to_save.parameters():\n",
    "            param.data = param.data.contiguous()\n",
    "\n",
    "        # Save model and tokenizer\n",
    "        model_to_save.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "def train_model(model, train_dataloader, validation_dataloader, optimizer, scheduler, tokenizer, epochs, device, output_dir):\n",
    "    training_stats = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        print(f\"\\n======== Epoch {epoch_i + 1} / {epochs} ========\")\n",
    "        print(\"Training...\")\n",
    "\n",
    "        avg_train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n",
    "        print(f\"\\n  Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        print(\"\\nRunning Validation...\")\n",
    "        avg_val_loss, avg_val_accuracy = validate_epoch(model, validation_dataloader, device)\n",
    "\n",
    "        training_stats.append({\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "        })\n",
    "\n",
    "        best_val_loss = save_best_model(model, tokenizer, output_dir, best_val_loss, avg_val_loss)\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "    return training_stats\n",
    "\n",
    "# Train the model\n",
    "output_dir = '/content/drive/MyDrive/best_model_save_lstm/'\n",
    "training_stats = train_model(model,\n",
    "                             train_dataloader,\n",
    "                             validation_dataloader,\n",
    "                             optimizer,\n",
    "                             scheduler,\n",
    "                             tokenizer,\n",
    "                             epochs=epochs,\n",
    "                             device=device,\n",
    "                             output_dir=output_dir)\n",
    "\n",
    "# Test evaluation function with comprehensive metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "def predict_and_evaluate(model, test_dataloader, device, dataset_name=\"Test\"):\n",
    "    model.eval()\n",
    "\n",
    "    total_test_loss = 0\n",
    "    total_test_accuracy = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=f\"Evaluating {dataset_name}\", total=len(test_dataloader)):\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "\n",
    "            loss = outputs['loss']\n",
    "            logits = outputs['logits']\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_test_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "            pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "            predictions.extend(pred_flat)\n",
    "            true_labels.extend(label_ids.flatten())\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
    "    avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"{dataset_name} evaluation completed in {epoch_time // 60:.0f}m {epoch_time % 60:.0f}s\")\n",
    "\n",
    "    # Calculate comprehensive metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted')\n",
    "    recall = recall_score(true_labels, predictions, average='weighted')\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "    # Also calculate metrics for each class\n",
    "    precision_per_class = precision_score(true_labels, predictions, average=None)\n",
    "    recall_per_class = recall_score(true_labels, predictions, average=None)\n",
    "    f1_per_class = f1_score(true_labels, predictions, average=None)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'loss': avg_test_loss\n",
    "    }\n",
    "\n",
    "    return predictions, true_labels, metrics\n",
    "\n",
    "def print_comprehensive_metrics(metrics, class_names, dataset_name):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{dataset_name.upper()} SET METRICS\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    print(f\"Overall Metrics:\")\n",
    "    print(f\"  Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
    "    print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Loss:      {metrics['loss']:.4f}\")\n",
    "\n",
    "    print(f\"\\nPer-Class Metrics:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"  {class_name}:\")\n",
    "        print(f\"    Precision: {metrics['precision_per_class'][i]:.4f}\")\n",
    "        print(f\"    Recall:    {metrics['recall_per_class'][i]:.4f}\")\n",
    "        print(f\"    F1-Score:  {metrics['f1_per_class'][i]:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_predictions, test_true_labels, test_metrics = predict_and_evaluate(\n",
    "    model, test_dataloader, device, \"Test Set\"\n",
    ")\n",
    "\n",
    "# Also evaluate on validation set for comparison\n",
    "print(\"\\nValidation Set Results (for comparison):\")\n",
    "val_predictions, val_true_labels, val_metrics = predict_and_evaluate(\n",
    "    model, validation_dataloader, device, \"Validation Set\"\n",
    ")\n",
    "\n",
    "# Updated class names - Changed from No Sarcasm/Sarcasm to Positive/Negative\n",
    "class_names = ['Positive', 'Negative']\n",
    "\n",
    "# Print comprehensive metrics\n",
    "print_comprehensive_metrics(test_metrics, class_names, \"Test\")\n",
    "print_comprehensive_metrics(val_metrics, class_names, \"Validation\")\n",
    "\n",
    "# Enhanced Confusion Matrix and Classification Report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_enhanced_confusion_matrix_and_report(predictions, true_labels, class_names, dataset_name, metrics):\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap='Blues', ax=axes[0], values_format='d')\n",
    "    axes[0].set_title(f\"Confusion Matrix - {dataset_name}\")\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    cm_normalized = confusion_matrix(true_labels, predictions, normalize='true')\n",
    "    disp_norm = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_names)\n",
    "    disp_norm.plot(cmap='Blues', ax=axes[1], values_format='.2f')\n",
    "    axes[1].set_title(f\"Normalized Confusion Matrix - {dataset_name}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Detailed classification report\n",
    "    report = classification_report(true_labels, predictions, target_names=class_names, digits=4)\n",
    "    print(f\"\\nDetailed Classification Report - {dataset_name}:\")\n",
    "    print(\"=\"*80)\n",
    "    print(report)\n",
    "\n",
    "    # Additional metrics summary\n",
    "    print(f\"\\nMetrics Summary - {dataset_name}:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Overall Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Macro Avg Precision: {np.mean(metrics['precision_per_class']):.4f}\")\n",
    "    print(f\"Macro Avg Recall: {np.mean(metrics['recall_per_class']):.4f}\")\n",
    "    print(f\"Macro Avg F1-Score: {np.mean(metrics['f1_per_class']):.4f}\")\n",
    "    print(f\"Weighted Avg Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Weighted Avg Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"Weighted Avg F1-Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "# Plot enhanced results for test set\n",
    "print(\"\\n\" + \"=\"*30 + \" TEST SET DETAILED RESULTS \" + \"=\"*30)\n",
    "plot_enhanced_confusion_matrix_and_report(test_predictions, test_true_labels, class_names, \"Test Set\", test_metrics)\n",
    "\n",
    "# Plot enhanced results for validation set\n",
    "print(\"\\n\" + \"=\"*30 + \" VALIDATION SET DETAILED RESULTS \" + \"=\"*30)\n",
    "plot_enhanced_confusion_matrix_and_report(val_predictions, val_true_labels, class_names, \"Validation Set\", val_metrics)\n",
    "\n",
    "# Enhanced training statistics with metrics comparison\n",
    "print(\"\\n\" + \"=\"*30 + \" TRAINING STATISTICS \" + \"=\"*30)\n",
    "training_df = pd.DataFrame(training_stats)\n",
    "print(training_df)\n",
    "\n",
    "# Create a comprehensive metrics comparison table\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Dataset': ['Validation', 'Test'],\n",
    "    'Accuracy': [val_metrics['accuracy'], test_metrics['accuracy']],\n",
    "    'Precision': [val_metrics['precision'], test_metrics['precision']],\n",
    "    'Recall': [val_metrics['recall'], test_metrics['recall']],\n",
    "    'F1-Score': [val_metrics['f1_score'], test_metrics['f1_score']],\n",
    "    'Loss': [val_metrics['loss'], test_metrics['loss']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*30 + \" METRICS COMPARISON \" + \"=\"*30)\n",
    "print(metrics_comparison.round(4))\n",
    "\n",
    "# Enhanced training progress plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Training and validation loss\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(training_df['epoch'], training_df['Training Loss'], 'b-o', label='Training Loss', linewidth=2)\n",
    "plt.plot(training_df['epoch'], training_df['Valid. Loss'], 'r-o', label='Validation Loss', linewidth=2)\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(training_df['epoch'], training_df['Valid. Accur.'], 'g-o', label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics comparison bar chart\n",
    "plt.subplot(2, 2, 3)\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "val_scores = [val_metrics['accuracy'], val_metrics['precision'], val_metrics['recall'], val_metrics['f1_score']]\n",
    "test_scores = [test_metrics['accuracy'], test_metrics['precision'], test_metrics['recall'], test_metrics['f1_score']]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, val_scores, width, label='Validation', alpha=0.8)\n",
    "plt.bar(x + width/2, test_scores, width, label='Test', alpha=0.8)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Validation vs Test Metrics Comparison')\n",
    "plt.xticks(x, metrics_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Per-class metrics comparison\n",
    "plt.subplot(2, 2, 4)\n",
    "class_metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "positive_scores = [test_metrics['precision_per_class'][0], test_metrics['recall_per_class'][0], test_metrics['f1_per_class'][0]]\n",
    "negative_scores = [test_metrics['precision_per_class'][1], test_metrics['recall_per_class'][1], test_metrics['f1_per_class'][1]]\n",
    "\n",
    "x = np.arange(len(class_metrics))\n",
    "plt.bar(x - width/2, positive_scores, width, label='Positive', alpha=0.8)\n",
    "plt.bar(x + width/2, negative_scores, width, label='Negative', alpha=0.8)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Per-Class Metrics (Test Set)')\n",
    "plt.xticks(x, class_metrics)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a final summary table\n",
    "final_results = {\n",
    "    'Model': ['BanglaBERT + CNN'],\n",
    "    'Dataset': ['Test Set'],\n",
    "    'Accuracy (%)': [f\"{test_metrics['accuracy']*100:.2f}%\"],\n",
    "    'Precision': [f\"{test_metrics['precision']:.4f}\"],\n",
    "    'Recall': [f\"{test_metrics['recall']:.4f}\"],\n",
    "    'F1-Score': [f\"{test_metrics['f1_score']:.4f}\"],\n",
    "    'Loss': [f\"{test_metrics['loss']:.4f}\"]\n",
    "}\n",
    "\n",
    "final_df = pd.DataFrame(final_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(final_df.to_string(index=False))\n",
    "\n",
    "# Per-class performance summary\n",
    "per_class_results = pd.DataFrame({\n",
    "    'Class': class_names,\n",
    "    'Precision': [f\"{score:.4f}\" for score in test_metrics['precision_per_class']],\n",
    "    'Recall': [f\"{score:.4f}\" for score in test_metrics['recall_per_class']],\n",
    "    'F1-Score': [f\"{score:.4f}\" for score in test_metrics['f1_per_class']]\n",
    "})\n",
    "\n",
    "print(f\"\\nPER-CLASS PERFORMANCE (TEST SET):\")\n",
    "print(\"-\" * 50)\n",
    "print(per_class_results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING AND EVALUATION COMPLETED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final Test Accuracy: {test_metrics['accuracy']:.4f} ({test_metrics['accuracy']*100:.2f}%)\")\n",
    "print(f\"Final Test Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Final Test Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"Final Test F1-Score: {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"Model saved to: {output_dir}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save metrics to CSV files for future reference\n",
    "training_df.to_csv('/content/drive/MyDrive/training_statistics.csv', index=False)\n",
    "metrics_comparison.to_csv('/content/drive/MyDrive/metrics_comparison.csv', index=False)\n",
    "final_df.to_csv('/content/drive/MyDrive/final_results.csv', index=False)\n",
    "per_class_results.to_csv('/content/drive/MyDrive/per_class_results.csv', index=False)\n",
    "\n",
    "print(f\"\\nResults saved to CSV files:\")\n",
    "print(\"- Training statistics: /content/drive/MyDrive/training_statistics.csv\")\n",
    "print(\"- Metrics comparison: /content/drive/MyDrive/metrics_comparison.csv\")\n",
    "print(\"- Final results: /content/drive/MyDrive/final_results.csv\")\n",
    "print(\"- Per-class results: /content/drive/MyDrive/per_class_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
